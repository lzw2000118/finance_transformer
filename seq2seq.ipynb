{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本文我们将采用带有注意力机制的seq2seq，用于金融数据的预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里是模型图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import datetime\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "![avatar](2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled dot-product attention\n",
    "对于查询向量Q，键K，值V，我们可以得到Scaled dot-product attention:\n",
    "$$Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaled_Dot_Product_Attention(nn.Module):\n",
    "    \"\"\"Scaled dot-product attention mechanism.\"\"\"\n",
    "\n",
    "    def __init__(self, attention_dropout=0.0):\n",
    "        super(Scaled_Dot_Product_Attention, self).__init__()\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v, scale=None, attn_mask=None):\n",
    "        \"\"\"前向传播.\n",
    "\n",
    "        Args:\n",
    "            q: Queries张量，形状为[B, L_q, D_q]\n",
    "            k: Keys张量，形状为[B, L_k, D_k]\n",
    "            v: Values张量，形状为[B, L_v, D_v]，一般来说就是k\n",
    "            上面的的B是Batch_size,L是序列长度，D为feature_size\n",
    "            scale: 缩放因子，一个浮点标量\n",
    "            attn_mask: Masking张量，形状为[B, L_q, L_k]\n",
    "\n",
    "        Returns:\n",
    "            上下文张量和attetention张量\n",
    "        \"\"\"\n",
    "        attention = torch.bmm(q, k.transpose(1, 2))# bmm:perform a batch matrix-matrix product of matrices\n",
    "        if scale:\n",
    "            attention = attention * scale\n",
    "        if attn_mask:\n",
    "            # 给需要mask的地方设置一个负无穷\n",
    "            attention = attention.masked_fill_(attn_mask, -np.inf)\n",
    "        # 计算softmax\n",
    "        attention = self.softmax(attention)\n",
    "        # 添加dropout\n",
    "        attention = self.dropout(attention)\n",
    "        # 和V做点积\n",
    "        context = torch.bmm(attention, v)\n",
    "        return context, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "q = Variable(torch.randn(5, 3, 10))\n",
    "k = Variable(torch.randn(5, 3, 10))\n",
    "v = Variable(torch.randn(5, 3, 10))\n",
    "attention = torch.bmm(q, k.transpose(1, 2))\n",
    "attention.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=2)\n",
    "dropout = nn.Dropout(0.2)\n",
    "attention1 = softmax(attention)#为（5,3,3）张量，可以知道（*，*，0）+（*，*，1）+（*，*，2）=1\n",
    "attention1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.bmm(attention1, v)\n",
    "context.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head attention\n",
    "所谓多头注意力，就是在上面的点积注意力基础上做了h次。两者的关系可以表示为：\n",
    "![avatar](4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Head_Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dim=512, num_heads=8, dropout=0.0):\n",
    "        \"\"\"\n",
    "        args:\n",
    "        model_dim:为多头注意力连接之后的维度\n",
    "        num_heads:多头数目\n",
    "        dropout:dropout率\n",
    "        \"\"\"\n",
    "        super(Multi_Head_Attention, self).__init__()\n",
    "\n",
    "        self.dim_per_head = model_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.linear_k = nn.Linear(model_dim, self.dim_per_head * num_heads)\n",
    "        self.linear_v = nn.Linear(model_dim, self.dim_per_head * num_heads)\n",
    "        self.linear_q = nn.Linear(model_dim, self.dim_per_head * num_heads)\n",
    "\n",
    "        self.dot_product_attention = Scaled_Dot_Product_Attention(dropout)\n",
    "        self.linear_final = nn.Linear(model_dim, model_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # multi-head attention之后需要做layer norm\n",
    "        self.layer_norm = nn.LayerNorm(model_dim)\n",
    "\n",
    "    def forward(self, key, value, query, attn_mask=None):\n",
    "        # 残差连接\n",
    "        residual = query\n",
    "\n",
    "        dim_per_head = self.dim_per_head\n",
    "        num_heads = self.num_heads\n",
    "        batch_size = key.size(0)\n",
    "\n",
    "        # linear projection\n",
    "        key = self.linear_k(key)\n",
    "        value = self.linear_v(value)\n",
    "        query = self.linear_q(query)\n",
    "\n",
    "        # split by heads\n",
    "        key = key.view(batch_size * num_heads, -1, dim_per_head)\n",
    "        value = value.view(batch_size * num_heads, -1, dim_per_head)\n",
    "        query = query.view(batch_size * num_heads, -1, dim_per_head)\n",
    "\n",
    "        if attn_mask:\n",
    "            attn_mask = attn_mask.repeat(num_heads, 1, 1)\n",
    "        # scaled dot product attention\n",
    "        scale = (key.size(-1) // num_heads) ** -0.5\n",
    "        context, attention = self.dot_product_attention(\n",
    "          query, key, value, scale, attn_mask)\n",
    "\n",
    "        # concat heads\n",
    "        context = context.view(batch_size, -1, dim_per_head * num_heads)\n",
    "\n",
    "        # final linear projection\n",
    "        output = self.linear_final(context)\n",
    "\n",
    "        # dropout\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        # add residual and norm layer\n",
    "        output = self.layer_norm(residual + output)\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = Multi_Head_Attention(dropout=0.1)\n",
    "query = Variable(torch.randn(5, 3, 512))\n",
    "key = Variable(torch.randn(5, 3, 512))\n",
    "value = Variable(torch.randn(5, 3, 512))\n",
    "output, attention = mha(key,value,query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 512])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 3, 3])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer normalization\n",
    "是在每一个样本上计算均值和方差：\n",
    "$$LN(x_i)=\\alpha\\times\\frac{x_i-\\mu_L}{\\sqrt{\\sigma^2_L+\\epsilon}}+\\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"实现LayerNorm。其实PyTorch已经实现啦，见nn.LayerNorm。\"\"\"\n",
    "\n",
    "    def __init__(self, features, epsilon=1e-6):\n",
    "        \"\"\"初始化\n",
    "\n",
    "        Args:\n",
    "            features: 就是模型的维度。论文默认512\n",
    "            epsilon: 一个很小的数，防止数值计算的除0错误\n",
    "        \"\"\"\n",
    "        super(LayerNorm, self).__init__()\n",
    "        # alpha\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\n",
    "        # beta\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"前向传播.\n",
    "\n",
    "        Args:\n",
    "            x: 输入序列张量，形状为[B, L, D]\n",
    "        \"\"\"\n",
    "        # 根据公式进行归一化\n",
    "        # 在X的最后一个维度求均值，最后一个维度就是模型的维度\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        # 在X的最后一个维度求方差，最后一个维度就是模型的维度\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.epsilon) + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mask \n",
    "顾名思义就是掩码，在我们这里的意思大概就是对某些值进行掩盖，使其不产生效果。\n",
    "\n",
    "Transformer模型里面涉及两种mask。分别是padding mask和sequence mask。其中，padding mask在所有的scaled dot-product attention里面都需要用到，而sequence mask只有在decoder的self-attention里面用到。\n",
    "### Padding mask\n",
    "我们的每个批次输入序列长度是不一样的！也就是说，我们要对输入序列进行对齐！具体来说，就是给在较短的序列后面填充0。因为这些填充的位置，其实是没什么意义的，所以我们的attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。\n",
    "\n",
    "具体的做法是，把这些位置的值加上一个非常大的负数(可以是负无穷)，这样的话，经过softmax，这些位置的概率就会接近0！\n",
    "### sequence mask\n",
    "sequence mask是为了使得decoder不能看见未来的信息。也就是对于一个序列，在time_step为t的时刻，我们的解码输出应该只能依赖于t时刻之前的输出，而不能依赖t之后的输出。因此我们需要想一个办法，把t之后的信息给隐藏起来。\n",
    "\n",
    "那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为1，下三角的值权威0，对角线也是0。把这个矩阵作用在每一个序列上，就可以达到我们的目的啦。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_mask(seq_k, seq_q):\n",
    "    # seq_k和seq_q的形状都是[B,L]\n",
    "    len_q = seq_q.size(1)\n",
    "    # `PAD` is 0\n",
    "    pad_mask = seq_k.eq(0)\n",
    "    pad_mask = pad_mask.unsqueeze(1).expand(-1, len_q, -1)  # shape [B, L_q, L_k]\n",
    "    return pad_mask\n",
    "\n",
    "def sequence_mask(seq):\n",
    "    batch_size, seq_len = seq.size()\n",
    "    mask = torch.triu(torch.ones((seq_len, seq_len), dtype=torch.uint8),\n",
    "                    diagonal=1)\n",
    "    mask = mask.unsqueeze(0).expand(batch_size, -1, -1)  # [B, L, L]\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a3b6394ffabe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mPositionalEncoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \"\"\"初始化。\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        \"\"\"初始化。\n",
    "        \n",
    "        Args:\n",
    "            d_model: 一个标量。模型的维度，论文默认是512\n",
    "            max_seq_len: 一个标量。文本序列的最大长度\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # 根据论文给的公式，构造出PE矩阵\n",
    "        position_encoding = np.array([\n",
    "          [pos / np.pow(10000, 2.0 * (j // 2) / d_model) for j in range(d_model)]\n",
    "          for pos in range(max_seq_len)])\n",
    "        # 偶数列使用sin，奇数列使用cos\n",
    "        position_encoding[:, 0::2] = np.sin(position_encoding[:, 0::2])\n",
    "        position_encoding[:, 1::2] = np.cos(position_encoding[:, 1::2])\n",
    "\n",
    "        # 在PE矩阵的第一行，加上一行全是0的向量，代表这`PAD`的positional encoding\n",
    "        # 在word embedding中也经常会加上`UNK`，代表位置单词的word embedding，两者十分类似\n",
    "        # 那么为什么需要这个额外的PAD的编码呢？很简单，因为文本序列的长度不一，我们需要对齐，\n",
    "        # 短的序列我们使用0在结尾补全，我们也需要这些补全位置的编码，也就是`PAD`对应的位置编码\n",
    "        pad_row = torch.zeros([1, d_model])\n",
    "        position_encoding = torch.cat((pad_row, position_encoding))\n",
    "        \n",
    "        # 嵌入操作，+1是因为增加了`PAD`这个补全位置的编码，\n",
    "        # Word embedding中如果词典增加`UNK`，我们也需要+1。看吧，两者十分相似\n",
    "        self.position_encoding = nn.Embedding(max_seq_len + 1, d_model)\n",
    "        self.position_encoding.weight = nn.Parameter(position_encoding,\n",
    "                                                     requires_grad=False)\n",
    "    def forward(self, input_len):\n",
    "        \"\"\"神经网络的前向传播。\n",
    "\n",
    "        Args:\n",
    "          input_len: 一个张量，形状为[BATCH_SIZE, 1]。每一个张量的值代表这一批文本序列中对应的长度。\n",
    "\n",
    "        Returns:\n",
    "          返回这一批序列的位置编码，进行了对齐。\n",
    "        \"\"\"\n",
    "        \n",
    "        # 找出这一批序列的最大长度\n",
    "        max_len = torch.max(input_len)\n",
    "        tensor = torch.cuda.LongTensor if input_len.is_cuda else torch.LongTensor\n",
    "        # 对每一个序列的位置进行对齐，在原序列位置的后面补上0\n",
    "        # 这里range从1开始也是因为要避开PAD(0)的位置\n",
    "        input_pos = tensor(\n",
    "          [list(range(1, len + 1)) + [0] * (max_len - len) for len in input_len])\n",
    "        return self.position_encoding(input_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position-wise Feed-Forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalWiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dim=512, ffn_dim=2048, dropout=0.0):\n",
    "        super(PositionalWiseFeedForward, self).__init__()\n",
    "        self.w1 = nn.Conv1d(model_dim, ffn_dim, 1)\n",
    "        self.w2 = nn.Conv1d(model_dim, ffn_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(model_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x.transpose(1, 2)\n",
    "        output = self.w2(F.relu(self.w1(output)))\n",
    "        output = self.dropout(output.transpose(1, 2))\n",
    "\n",
    "        # add residual and norm layer\n",
    "        output = self.layer_norm(x + output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Encoder的一层。\"\"\"\n",
    "\n",
    "    def __init__(self, model_dim=512, num_heads=8, ffn_dim=2018, dropout=0.0):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(model_dim, num_heads, dropout)\n",
    "        self.feed_forward = PositionalWiseFeedForward(model_dim, ffn_dim, dropout)\n",
    "\n",
    "    def forward(self, inputs, attn_mask=None):\n",
    "\n",
    "        # self attention\n",
    "        context, attention = self.attention(inputs, inputs, inputs, padding_mask)\n",
    "\n",
    "        # feed forward network\n",
    "        output = self.feed_forward(context)\n",
    "\n",
    "        return output, attention\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"多层EncoderLayer组成Encoder。\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "               vocab_size,\n",
    "               max_seq_len,\n",
    "               num_layers=6,\n",
    "               model_dim=512,\n",
    "               num_heads=8,\n",
    "               ffn_dim=2048,\n",
    "               dropout=0.0):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "          [EncoderLayer(model_dim, num_heads, ffn_dim, dropout) for _ in\n",
    "           range(num_layers)])\n",
    "\n",
    "        self.seq_embedding = nn.Embedding(vocab_size + 1, model_dim, padding_idx=0)\n",
    "        self.pos_embedding = PositionalEncoding(model_dim, max_seq_len)\n",
    "\n",
    "    def forward(self, inputs, inputs_len):\n",
    "        output = self.seq_embedding(inputs)\n",
    "        output += self.pos_embedding(inputs_len)\n",
    "\n",
    "        self_attention_mask = padding_mask(inputs, inputs)\n",
    "\n",
    "        attentions = []\n",
    "        for encoder in self.encoder_layers:\n",
    "            output, attention = encoder(output, self_attention_mask)\n",
    "            attentions.append(attention)\n",
    "\n",
    "        return output, attentions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dim, num_heads=8, ffn_dim=2048, dropout=0.0):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(model_dim, num_heads, dropout)\n",
    "        self.feed_forward = PositionalWiseFeedForward(model_dim, ffn_dim, dropout)\n",
    "\n",
    "    def forward(self,\n",
    "              dec_inputs,\n",
    "              enc_outputs,\n",
    "              self_attn_mask=None,\n",
    "              context_attn_mask=None):\n",
    "        # self attention, all inputs are decoder inputs\n",
    "        dec_output, self_attention = self.attention(\n",
    "          dec_inputs, dec_inputs, dec_inputs, self_attn_mask)\n",
    "\n",
    "        # context attention\n",
    "        # query is decoder's outputs, key and value are encoder's inputs\n",
    "        dec_output, context_attention = self.attention(\n",
    "          enc_outputs, enc_outputs, dec_output, context_attn_mask)\n",
    "\n",
    "        # decoder's output, or context\n",
    "        dec_output = self.feed_forward(dec_output)\n",
    "\n",
    "        return dec_output, self_attention, context_attention\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "               vocab_size,\n",
    "               max_seq_len,\n",
    "               num_layers=6,\n",
    "               model_dim=512,\n",
    "               num_heads=8,\n",
    "               ffn_dim=2048,\n",
    "               dropout=0.0):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "          [DecoderLayer(model_dim, num_heads, ffn_dim, dropout) for _ in\n",
    "           range(num_layers)])\n",
    "\n",
    "        self.seq_embedding = nn.Embedding(vocab_size + 1, model_dim, padding_idx=0)\n",
    "        self.pos_embedding = PositionalEncoding(model_dim, max_seq_len)\n",
    "\n",
    "    def forward(self, inputs, inputs_len, enc_output, context_attn_mask=None):\n",
    "        output = self.seq_embedding(inputs)\n",
    "        output += self.pos_embedding(inputs_len)\n",
    "\n",
    "        self_attention_padding_mask = padding_mask(inputs, inputs)\n",
    "        seq_mask = sequence_mask(inputs)\n",
    "        self_attn_mask = torch.gt((self_attention_padding_mask + seq_mask), 0)\n",
    "\n",
    "        self_attentions = []\n",
    "        context_attentions = []\n",
    "        for decoder in self.decoder_layers:\n",
    "            output, self_attn, context_attn = decoder(\n",
    "            output, enc_output, self_attn_mask, context_attn_mask)\n",
    "            self_attentions.append(self_attn)\n",
    "            context_attentions.append(context_attn)\n",
    "\n",
    "        return output, self_attentions, context_attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "               src_vocab_size,\n",
    "               src_max_len,\n",
    "               tgt_vocab_size,\n",
    "               tgt_max_len,\n",
    "               num_layers=6,\n",
    "               model_dim=512,\n",
    "               num_heads=8,\n",
    "               ffn_dim=2048,\n",
    "               dropout=0.2):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(src_vocab_size, src_max_len, num_layers, model_dim,\n",
    "                               num_heads, ffn_dim, dropout)\n",
    "        self.decoder = Decoder(tgt_vocab_size, tgt_max_len, num_layers, model_dim,\n",
    "                               num_heads, ffn_dim, dropout)\n",
    "\n",
    "        self.linear = nn.Linear(model_dim, tgt_vocab_size, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, src_seq, src_len, tgt_seq, tgt_len):\n",
    "        context_attn_mask = padding_mask(tgt_seq, src_seq)\n",
    "\n",
    "        output, enc_self_attn = self.encoder(src_seq, src_len)\n",
    "\n",
    "        output, dec_self_attn, ctx_attn = self.decoder(\n",
    "          tgt_seq, tgt_len, output, context_attn_mask)\n",
    "\n",
    "        output = self.linear(output)\n",
    "        output = self.softmax(output)\n",
    "\n",
    "        return output, enc_self_attn, dec_self_attn, ctx_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建基于GRU的编码器\n",
    " 编码器结构图:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "在金融数据中，词嵌入层是否需要，Embedding的含义我需要搞清楚\n",
    "'''\n",
    "# Encoder 部分\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,num_layers,seq_len,batch_num):\n",
    "        \"\"\"它的初始化参数有两个, input_size代表编码器的输入尺寸即源语言的，hidden_size代表GRU的隐层节点数, 同时又是GRU的输入尺寸\"\"\"\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size   # 隐藏层特征\n",
    "        self.seq_len = seq_len   #序列长度\n",
    "        self.batch_num = batch_num  #一批同时训练数据的数目\n",
    "        self.num_layers = num_layers # GRU层数\n",
    "        self.gru = nn.GRU(input_size, hidden_size,num_layers)   # 多层的GRU\n",
    "        \n",
    "    def forward(self, Input, hidden):\n",
    "        \"\"\"编码器前向逻辑函数中参数有两个, input代表输入张量\n",
    "           hidden代表编码器层gru的初始隐层张量\"\"\"\n",
    "        # Input为三维张量:(seq_len, batch_num, input_size),seq_len为序列长度，batch_num为一批同时训练多少数据，input_size为输入的特征\n",
    "        #输出output为三维张量:(seq_len, batch_num,  hidden_size),seq_len为序列长度，batch_num为一批同时训练多少数据，output_size为输入的特征\n",
    "        #隐藏层hidden为三维张量:(num_layers, batch_num,  hidden_size),num_layers为隐藏层层数，batch_num为一批同时训练多少数据，input_size为输入的特征\n",
    "        output, hidden = self.gru(Input, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        \"\"\"初始化隐层张量函数\"\"\"\n",
    "        return torch.zeros(self.num_layers,self.batch_num, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "# Decoder部分\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "         \"\"\"初始化函数有两个参数，hidden_size代表解码器中GRU的输入尺寸，也是它的隐层节点数\n",
    "           output_size代表整个解码器的输出尺寸,\"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # 将hidden_size传入到类中# 将hidden_size传入到类中\n",
    "        self.hidden_size = hidden_size\n",
    "        # 实例化GRU对象，输入参数都是hidden_size，代表它的输入尺寸和隐层节点数相同\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        # 实例化线性层, 对GRU的输出做线性变化, 获我们希望的输出尺寸output_size\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, Input, hidden):\n",
    "        output, hidden = self.gru(Iutput, hidden)\n",
    "        output = self.out(output[0])\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,1,self.hidden_size, device=device)\n",
    "\n",
    "# Attention 部分\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        \"\"\"初始化函数中的参数有4个, hidden_size代表解码器中GRU的输入尺寸，也是它的隐层节点数\n",
    "           output_size代表整个解码器的输出尺寸,\n",
    "           dropout_p代表我们使用dropout层时的置零比率，默认0.1, max_length代表句子的最大长度\"\"\"\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        # 将以下参数传入类中\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size*2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size*2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "    \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)  # 注意力权重\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))  # 两个batch之间的矩阵乘法\n",
    "        \n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        \n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建基于GRU的解码器\n",
    "解码器结构图:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 将法语翻译成英语\n",
    "\n",
    "\n",
    "SOS_token = 0  # 开始的标注\n",
    "EOS_token = 1  # 结束的标注\n",
    "\n",
    "# 辅助类\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}   # word---->index\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}  # index---->word\n",
    "        self.word2count = {}   # 稍后用来替换稀有单词，统计每个单词出现的次数\n",
    "        self.n_words = 2  # 统计单词总数\n",
    "    \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n",
    "    \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    " # Turn a Unicode string to plain ASCII\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# 小写，修剪和删除非字母字符\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "# 加载文件\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines.......\")\n",
    "    # 读取文件并进行划分成行\n",
    "    lines = open(r\"E://DeepLearning//jupyter_code//dataset//corpus//translation_data//%s-%s.txt\" % (lang1, lang2), encoding='utf-8').\\\n",
    "                read().strip().split(\"\\n\")\n",
    "    \n",
    "    # 将每行切成一组pairs\n",
    "    pairs = [[normalizeString(s) for s in l.split(\"\\t\")] for l in lines]\n",
    "    # 将其他语言翻译成英语\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)  \n",
    "        output_lang = Lang(lang2)   \n",
    "        \n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "# 由于有很多例句，为了能快速训练，我们会将数据集修剪成相对简短的句子。这里最大长度是10个单词（包括结束标点符号）\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "# 英语前缀\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)    \n",
    "# print(\"pairs:\\n\", pairs)  pairs = [法语,英语]\n",
    "print(random.choice(pairs))\n",
    "\n",
    "\n",
    "# Encoder 部分\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size   # 隐藏状态a的大小\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)  # 词嵌入层\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)   # 多层的GRU\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,1, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "# Decoder部分\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,1,self.hidden_size, device=device)\n",
    "\n",
    "# Attention 部分\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size*2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size*2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "    \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)  # 注意力权重\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))  # 两个batch之间的矩阵乘法\n",
    "        \n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        \n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    # 隐状态初始化\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "# 训练模型\n",
    "\n",
    "# 准备训练数据\n",
    "\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(\" \")]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)   # EOS作为encoder编码器网络的结束标志，  SOS作为Decoder解码器网络的开始标志\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair): \n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])   # pair[0]是法语\n",
    "    targe_tensor = tensorFromSentence(output_lang, pair[1])  # pair[1]是英语\n",
    "    return (input_tensor, targe_tensor)\n",
    "\n",
    "\n",
    "# 开始训练\n",
    "\n",
    "# “tearcher_forcing_ratio将上一时刻的真实目标输出当作下一个时刻的Encoder网络的输入，而不是使用Encoder网络的上一时刻的预测输出作为下一时刻的输入。\n",
    "tearcher_forcing_ratio = 0.5  \n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    # encoder部分\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "    \n",
    "    # decoder部分\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    use_teacher_foring = True if random.random() < tearcher_forcing_ratio else False\n",
    "    \n",
    "    # using teacher forcing\n",
    "    if use_teacher_foring:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  \n",
    "            \n",
    "    # 不使用teacher forcing,使用上一时刻的输出作为下一时刻的输入        \n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  \n",
    "            \n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            \n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item() / target_length\n",
    "\n",
    "\n",
    "# 辅助函数------记录时间\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (- %s)\" % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "# 整个训练过程如下：\n",
    " # 开启定时器\n",
    " # 初始化优化器和loss函数\n",
    " # 创建training pairs\n",
    " # 开始训练并绘图\n",
    "\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()  # 开启定时器\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)  # 定义优化算法\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))  # 创建training pairs\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()  # 定义损失函数\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "\n",
    "\n",
    "# 绘制loss曲线\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "\n",
    "# 测试阶段--------测试阶段整体与训练阶段类似，但是测试阶段，不用给出target_tensor,只是将decoder网络上一时刻的预测值作为下一时刻的输入值\n",
    "# 当预测值是EOS时，则停止预测\n",
    "\n",
    "def evaluate(encoder, decoder, sentence, max_length = MAX_LENGTH):\n",
    "     with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        # encoder部分\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "        \n",
    "        # decoder部分\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:  # 结束时的条件\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    "\n",
    "# 随机地从训练集中选择pairs,然后在测试集上进行评估\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('输入:>', pair[0])\n",
    "        print('目标:=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('预测:<', output_sentence)\n",
    "        print('')\n",
    "\n",
    "# 正式训练开始运行\n",
    "\n",
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)\n",
    "\n",
    "evaluateRandomly(encoder1, attn_decoder1)\n",
    "\n",
    "# 注意力可视化\n",
    "\n",
    "output_words, attentions = evaluate(\n",
    "    encoder1, attn_decoder1, \"je suis trop froid .\")\n",
    "plt.matshow(attentions.numpy());\n",
    "\n",
    "# 增加坐标轴，更加清楚的可视化\n",
    "\n",
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# benchmark\n",
    "基础的\n",
    "\n",
    "![avatar](5.png)\n",
    "\n",
    "堆栈的两层结构为：\n",
    "\n",
    "![avatar](8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一种一对多的含注意力机制的模型\n",
    "提出两种结构，分别为Ⅰ和Ⅱ型。Ⅰ型相对简单，Ⅱ型较为复杂。两者模型结构图\n",
    "\n",
    "![avatar](3.png)\n",
    "\n",
    "![avatar](7.png)\n",
    "\n",
    "模型中的GA_cell是gru_attention_cell,通过在单元中引入注意力机制来获得其他任务中的信息。\n",
    "\n",
    "对应的不包含注意力机制的模型如下：\n",
    "\n",
    "![avatar](6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq_rnn(nn.Module):\n",
    "    \"\"\"这是benchmark\"\"\"\n",
    "\n",
    "    def __init__(self,input_size,hidden_size,output_size=1,result_nums=2,dropout=0.1):\n",
    "        super(seq2seq_rnn, self).__init__()\n",
    "        self.input_size = input_size #输入数据特征数目\n",
    "        self.hidden_size = hidden_size #隐藏层特征数目\n",
    "        self.output_size = output_size #输出层特征数目\n",
    "        self.result_nums = result_nums #Decoder返回的结果数目\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.encoder_layer1 = nn.GRU(input_size,hidden_size,1,dropout=dropout)\n",
    "        self.encoder_layer2 = nn.GRU(hidden_size,output_size,1,dropout=dropout)\n",
    "        self.decoder_layer1 = nn.GRU(output_size,hidden_size,1,dropout=dropout)\n",
    "        self.decoder_layer2 = nn.GRU(hidden_size,output_size,1,dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"前向传播.\n",
    "\n",
    "        Args:\n",
    "            x: 输入张量，形状为[seq_len , batch , input_size]\n",
    "        Returns:\n",
    "            y\n",
    "        \"\"\"\n",
    "        x,h1 = self.encoder_layer1(x) # x:(seq_len, batch, hidden_size),h1:(1, batch, hidden_size)\n",
    "        x,h2 = self.encoder_layer2(x) # x:(seq_len, batch, output_size),h2:(1, batch, output_size)\n",
    "        out = h2.repeat(self.result_nums,1,1) # out:(result_nums, batch, output_size)\n",
    "        y,_ = self.decoder_layer1(out,h1) # y:(result_nums, batch, hidden_size)\n",
    "        y,_ = self.decoder_layer2(y,h2) # y:(result_nums, batch, output_size)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3,5,10)\n",
    "rnn = seq2seq_rnn(10,20)\n",
    "y = rnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OSMG(nn.Module):\n",
    "    __constants__ = [ 'input_size', 'hidden_size', 'task_nums', 'bias',\n",
    "                     'batch_first', 'dropout']\n",
    "\n",
    "    def __init__(self, input_size, hidden_size,task_nums,result_nums,\n",
    "                bias=True, batch_first=False,\n",
    "                 dropout=0.):\n",
    "        super(OSMG, self).__init__()\n",
    "        self.input_size = input_size #输入数据特征数目\n",
    "        self.hidden_size = hidden_size #隐藏状态特征数目\n",
    "        self.task_nums = task_nums #表示的是任务数量\n",
    "        self.bias = bias\n",
    "        self.batch_first = batch_first\n",
    "        self.dropout = dropout\n",
    "        self.models = nn.ModuleList([seq2seq_rnn(input_size,hidden_size,result_nums,dropout) for i in range(task_nums)])\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "        x:输入张量，形状为[seq_len , batch , input_size]\n",
    "        \n",
    "        返回值：\n",
    "        y:list\n",
    "        \"\"\"\n",
    "        y = []\n",
    "        for model in self.models:\n",
    "            y_i = model(x)\n",
    "            y.append(y_i)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OSMGIAM-I的Encoder_base结构\n",
    "\n",
    "![avatar](9.png)\n",
    "\n",
    "Decoder_base结构\n",
    "\n",
    "![avatar](10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_base(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size,dropout):\n",
    "        super(Encoder_base,self).__init__()\n",
    "        self.input_size = input_size #输入数据特征数目\n",
    "        self.hidden_size = hidden_size #隐藏层特征数目\n",
    "        self.output_size = output_size #输出层特征数目\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.layer1 = nn.GRU(input_size,hidden_size,1,dropout=dropout)\n",
    "        self.layer2 = nn.GRU(hidden_size,output_size,1,dropout=dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "        x:输入张量，形状为[seq_len , batch , input_size]\n",
    "        \n",
    "        返回值：\n",
    "        \n",
    "        \"\"\"\n",
    "        x,h1 = self.layer1(x) # x:(seq_len, batch, hidden_size),h1:(1, batch, hidden_size)\n",
    "        _,h2 = self.layer2(x) # h2:(1, batch, output_size)\n",
    "        return h1,h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_base(nn.Module):\n",
    "    def __init__(self,hidden_size,output_size,dropout):\n",
    "        super(Decoder_base,self).__init__()\n",
    "        self.hidden_size = hidden_size #隐藏层特征数目\n",
    "        self.output_size = output_size #输出层特征数目\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.layer1 = nn.GRU(2*output_size,2*hidden_size,1,dropout=dropout)\n",
    "        self.layer2 = nn.GRU(2*hidden_size,output_size,1,dropout=dropout)\n",
    "        \n",
    "    def forward(self,x,h1,h2):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "        x:输入张量，形状为[seq_len , batch , 2*output_size]\n",
    "        h1隐藏状态张量，形状为[1 , batch , 2*hidden_size]\n",
    "        h2隐藏状态张量，形状为[1 , batch , output_size]\n",
    "        返回值：\n",
    "        \n",
    "        \"\"\"\n",
    "        x,_ = self.layer1(x,h1) # x:(seq_len, batch, 2*hidden_sizeh)\n",
    "        x,_ = self.layer2(x,h2) # x:(seq_len, batch, output_size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OSMGIAM1(nn.Module):\n",
    "    __constants__ = [ 'input_size', 'hidden_size', 'task_nums', 'bias',\n",
    "                     'batch_first', 'dropout']\n",
    "\n",
    "    def __init__(self, input_size, hidden_size,output_size,task_nums,result_nums,\n",
    "                bias=True, batch_first=False,\n",
    "                 dropout=0.):\n",
    "        super(OSMGIAM1, self).__init__()\n",
    "        self.input_size = input_size #输入数据特征数目\n",
    "        self.hidden_size = hidden_size #隐藏状态特征数目\n",
    "        self.output_size = output_size #输出状态特征数目\n",
    "        self.task_nums = task_nums #表示的是任务数量\n",
    "        self.result_nums = result_nums\n",
    "        self.bias = bias\n",
    "        self.batch_first = batch_first\n",
    "        self.dropout = dropout\n",
    "        self.dot_product_attention = Scaled_Dot_Product_Attention(dropout)\n",
    "        self.encoders = nn.ModuleList([Encoder_base(input_size,hidden_size,output_size,dropout) for i in range(task_nums)])\n",
    "        self.decoders = nn.ModuleList([Decoder_base(hidden_size,output_size,dropout) for i in range(task_nums)])\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "        x:输入张量，形状为[seq_len , batch , input_size]\n",
    "        \"\"\"\n",
    "        _,b,_ = x.size()\n",
    "        H1 = torch.zeros(self.task_nums,b,self.hidden_size)\n",
    "        H2 = torch.zeros(self.task_nums,b,self.output_size)\n",
    "        i = 0\n",
    "        for encoder in self.encoders:\n",
    "            h1,h2 = encoder(x)\n",
    "            H1[i] = h1.reshape(b,self.hidden_size)\n",
    "            H2[i] = h2.reshape(b,self.output_size)\n",
    "            i += 1\n",
    "        H1 = H1.permute(1,0,2) # (batch,task,hidden)\n",
    "        H2 = H2.permute(1,0,2) # (batch,task,output)\n",
    "        context1,attention1 = self.dot_product_attention(H1,H1,H1) # context1:(batch,task,hidden)\n",
    "        context2,attention2 = self.dot_product_attention(H2,H2,H2) # context2:(batch,task,output)\n",
    "        H1 = H1.permute(1,0,2) # (task,batch,hidden)\n",
    "        H2 = H2.permute(1,0,2) # (task,batch,output)\n",
    "        context1 = context1.permute(1,0,2) # (task,batch,hidden)\n",
    "        H1 = torch.cat([H1,context1],dim=2) # (task,batch,2*hidden)\n",
    "        context2 = context2.permute(1,0,2) # (task,batch,output)\n",
    "        H = torch.cat([H2,context2],dim=2) # (task,batch,2*output)\n",
    "        j = 0\n",
    "        Y = []\n",
    "        for decoder in self.decoders:\n",
    "            temp = H[j].reshape(1,b,2*self.output_size)\n",
    "            out = temp.repeat(self.result_nums,1,1) # out:(result_nums, batch,2*output_size)\n",
    "            h1_temp = H1[j].reshape(1,b,2*self.hidden_size) # (1, batch, 2*hidden_size)\n",
    "            h2_temp = H2[j].reshape(1,b,self.output_size) # (1, batch, output_size)\n",
    "            y = decoder(out,h1_temp,h2_temp) # y:(result_nums, batch, output_size)\n",
    "            Y.append(y)\n",
    "            j += 1\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GA_cell\n",
    "![avatar](1.png)\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}{l}\n",
    "r_{t, k}=\\sigma\\left(W_{r} x_{t, k}+U_{r} h_{t, k-1}+A_{r} a_{t, k}+b_{r}\\right) \\\\\n",
    "z_{t, k}=\\sigma\\left(W_{z} x_{t, k}^{i}+U_{z} h_{t, k-1}+A_{z} a_{t, k}+b_{z}\\right) \\\\\n",
    "\\widetilde{h_{t, k}}=tanh\\left(W x_{t, k}+U\\left[r_{t, k}^{i} \\cdot h_{t, k-1}^{i}\\right]+A a_{t, k}+b\\right), \\\\\n",
    "h_{t, k}=\\left(1-z_{t, k}\\right) \\cdot h_{t, k-1}+z_{t, k} \\cdot \\widetilde{h_{t, k}},\n",
    "\\end{array}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers\n",
    "\n",
    "class OSMGIAM2(nn.Module):\n",
    "    __constants__ = [ 'input_size', 'hidden_size', 'task_nums', 'bias',\n",
    "                     'batch_first', 'dropout']\n",
    "\n",
    "    def __init__(self, input_size, hidden_size,num_tasks,\n",
    "                bias=True, batch_first=False,\n",
    "                 dropout=0.):\n",
    "        super(OSMGIAM2, self).__init__()\n",
    "        self.input_size = input_size #输入数据特征数目\n",
    "        self.hidden_size = hidden_size #隐藏状态特征数目\n",
    "        self.task_nums = task_nums #表示的是任务数量\n",
    "        self.bias = bias\n",
    "        self.batch_first = batch_first\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dot_product_attention = Scaled_Dot_Product_Attention(dropout) #点积注意力模型\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # 与输入相关的参数\n",
    "        self.W_r = Parameter(torch.Tensor(task_nums, hidden_size, input_size))\n",
    "        self.W_z = Parameter(torch.Tensor(task_nums, hidden_size, input_size))\n",
    "        self.W_h =  Parameter(torch.Tensor(task_nums, hidden_size, input_size))\n",
    "        \n",
    "        # 与隐藏状态相关的参数\n",
    "        self.U_r = Parameter(torch.Tensor(task_nums, hidden_size, hidden_size))\n",
    "        self.U_z = Parameter(torch.Tensor(task_nums, hidden_size, hidden_size))\n",
    "        self.U_h =  Parameter(torch.Tensor(task_nums, hidden_size, hidden_size))\n",
    "        \n",
    "        #与注意力相关的参数\n",
    "        self.A_r = Parameter(torch.Tensor(task_nums, hidden_size, hidden_size))\n",
    "        self.A_z = Parameter(torch.Tensor(task_nums, hidden_size, hidden_size))\n",
    "        self.A_h =  Parameter(torch.Tensor(task_nums, hidden_size, hidden_size))\n",
    "        \n",
    "        #偏置\n",
    "        self.b_r = Parameter(torch.Tensor(task_nums, hidden_size))\n",
    "        self.b_z = Parameter(torch.Tensor(task_nums, hidden_size))\n",
    "        self.b_h = Parameter(torch.Tensor(task_nums, hidden_size))\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "        x:输入张量，形状为[seq_len , batch , input_size]\n",
    "        \"\"\"\n",
    "        #定义一个张量H和A，其中H用来存储上一时刻的隐藏状态，而A用来存储上一时刻的注意力值。H:(task_nums,batch,hidden_size),A:(task_nums,batch,hidden_size)\n",
    "        seq_len , b , _  = x.size()\n",
    "        H = torch.zeros(self.task_nums,b,self.hidden_size)\n",
    "        A = torch.zeros(self.task_nums,b,self.hidden_size)\n",
    "        for i in range(seq_len):\n",
    "            H_temp = torch.ones(self.task_nums,b,self.hidden_size)\n",
    "            for j in range(self.task_nums):\n",
    "                r_j = self.sigmoid(torch.mm(self.W_r[j],x[i].T) + torch.mm(self.U_r[j],H[j].T) + torch.mm(self.A_r[j],A[j].T) + self.b_r[j].repeat(b,1).T) # r_j:(hidden_size,batch)\n",
    "                z_j = self.sigmoid(torch.mm(self.W_z[j],x[i].T) + torch.mm(self.U_z[j],H[j].T) + torch.mm(self.A_z[j],A[j].T) + self.b_z[j].repeat(b,1).T) # z_j:(hidden_size,batch)\n",
    "                h_tilde = self.tanh(torch.mm(self.W_h[j],x[i].T) + torch.mm(self.U_h[j],r_j.mul(H[0].T)) + torch.mm(self.A_h[j],A[j].T) + self.b_h[j].repeat(b,1).T) # h_tilde:(hidden_size,batch)\n",
    "                h_j = (1-z_j).mul(H[j].T) + z_j.mul(h_tilde) # h_j:(hidden_size,batch)\n",
    "                H_temp[j] = h_j.T\n",
    "            H = H_temp\n",
    "            H_temp = H_temp.permute(1,0,2)\n",
    "            A,_ = self.dot_product_attention(H_temp,H_temp,H_temp)\n",
    "            A = A.permute(1,0,2) #(task_nums,b,hidden_size)\n",
    "        return H,A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = nn.GRU(10,20,6)\n",
    "Input = Variable(torch.randn(5, 4, 10))#Input的维度：(seq_len, batch, input_size)\n",
    "h0 = Variable(torch.randn(6, 4, 20)) #初始化h0 维度：(layer_nums, batch, output_size)\n",
    "out,h = gru(Input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = torch.zeros(task_nums,b,hidden_size)\n",
    "A = torch.zeros(task_nums,b,hidden_size)\n",
    "for i in range(seq_len):\n",
    "    H_temp = torch.ones(task_nums,b,hidden_size)\n",
    "    for j in range(task_nums):\n",
    "        r_j = sigmoid(torch.mm(W_r[j],x[i].T) + torch.mm(U_r[j],H[j].T) + torch.mm(A_r[j],A[j].T) + b_r[j].repeat(b,1).T) # r_j:(hidden_size,batch)\n",
    "        z_j = sigmoid(torch.mm(W_z[j],x[i].T) + torch.mm(U_z[j],H[j].T) + torch.mm(A_z[j],A[j].T) + b_z[j].repeat(b,1).T) # z_j:(hidden_size,batch)\n",
    "        h_tilde = tanh(torch.mm(W_h[j],x[i].T) + torch.mm(U_h[j],r_j.mul(H[0].T)) + torch.mm(A_h[j],A[j].T) + b_h[j].repeat(b,1).T) # h_tilde:(hidden_size,batch)\n",
    "        h_j = (1-z_j).mul(H[j].T) + z_j.mul(h_tilde) # h_j:(hidden_size,batch)\n",
    "        H_temp[j] = h_j.T\n",
    "    H = H_temp #(task_nums,b,hidden_size)\n",
    "    H_temp = H_temp.permute(1,0,2) # \n",
    "    A,_ = dot_product_attention(H_temp,H_temp,H_temp) # A:(b,task_nums,hidden_size)\n",
    "    A = A.permute(1,0,2) #(task_nums,b,hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#订单付款时间，拆分成：month,day,weekday,hour,minute,second\n",
    "import datetime\n",
    "def time2multi(x):\n",
    "    t = datetime.datetime.strptime(x,'%Y-%m-%d %H:%M:%S')\n",
    "    return pd.Series([t.month,t.day,t.weekday(),t.hour,t.minute,t.second])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
